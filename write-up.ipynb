{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c82163",
   "metadata": {},
   "source": [
    "\tThe first step of the process was importing the data. I had the salary data and the NBA stats data. From there, I merged the two datasets by player name so that I could work with all of the data at the same time. Once I merged the two datasets, I dropped the rows without salary data because I decided that there would be no reason to predict players who did not have a salary value available. However, there was a problem of duplicate player names in the dataset. I considered multiple options including summing the rows of multiple player names and then averaging statistics and simply dropping the rows where the time played in games is less. I decided to go with the latter option because many of the duplicate rows seemed as though they had nearly identical information, which would not change the predictions that I was going to make later on anyways. To do this, I used a function that grouped by the player name and then took the max for the column called Minutes Played so that it was only taking the row with the maximum time on the court.\n",
    "\t\n",
    "    At this point, I just needed to finish preprocessing the data by normalizing all the numeric columns so that they would be comparable when I modeled them later. To do this, I gathered all the numeric columns and then I put them through a scikit-learn function called MinMaxScaler to make the data fall between 0 and 1. 1 would be the maximum for that column and 0 would be the minimum for that column. \n",
    "\n",
    "\tThen, I made a guess that the best number of clusters would be 2, so that is what I put in the KMeans function to create the model. The columns of interest I decided would be best are Rank, Minutes Played, and Points. From there, I found the coordinates of the cluster centers and printed them, along with the cluster that each player was assigned to, and finally the within-cluster sum of squares value which measured how tight or compact the clusters are. I created a visualization of the variable that I had thought would be best to differentiate the clusters and then evaluated the quality through multiple means. I calculated the variance explained value by taking the between sum of squares value (which calculates the distance from clusters) and the total sum of squares value (which calculates sum of squares inside the clusters and between the clusters) and found that it was about 71%. I also looked at the silhouette scores which told me that the best number of clusters would be 2. I also looked at an elbow plot which showed me that the most improvement to the clusters would occur at 2 clusters. \n",
    "    \n",
    "\tWith that done, I calculated the KMeans clustering with 2 clusters again and visualized it with a graph. Then I picked the good players by looking at the upper right cluster and finding players with lower salaries. I picked bad players by looking at high salary players that fell on the bottom left cluster. Finally, I picked could work players by looking at middle of the pack players with lower salaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b891af7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
